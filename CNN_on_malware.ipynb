{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN on malware.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMMbmOZjYXufFSGApGxPKhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyanji/Security/blob/master/CNN_on_malware.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-p9UyNallIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3uSrxmoP2W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlPFRyiLU4DZ",
        "colab_type": "text"
      },
      "source": [
        "## Conversion from data to pixel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQyGF2qkTDF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvPixel(FVec, xp, yp, A, B, base=1):\n",
        "    n = len(FVec)\n",
        "    M = np.ones([int(A), int(B)]) * base\n",
        "    for j in range(0, n):\n",
        "        M[int(xp[j]) - 1, int(yp[j]) - 1] = FVec[j]\n",
        "    zp = np.array([xp, yp])\n",
        "    dup ={}\n",
        "    # find duplicate\n",
        "    for i in range(len(zp[0, :])):\n",
        "        for j in range(i + 1, len(zp[0])):\n",
        "            if zp[0, i] == zp[0, j] and zp[1, i] == zp[1, j]:\n",
        "                #if i in dup.keys():\n",
        "                #print(\"duplicate:\" + str(i) + \" \" + str(j)+ \"value: \")\n",
        "                #dup.add(i)\n",
        "                #dup[i].add(j)\n",
        "                dup.setdefault(str(zp[0, i])+\"-\"+str(zp[1, i]), {i}).add(j)\n",
        "    #print(\"Collisioni:\"+str(len(dup.keys())))\n",
        "    for index in dup.keys():\n",
        "        x,y=index.split(\"-\")\n",
        "        M[int(float(x)) - 1, int(float(y)) - 1] = sum(FVec[list(dup[index])])/len(dup[index])\n",
        "    return M\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzzDabKQVT0Z",
        "colab_type": "text"
      },
      "source": [
        "## Min Rect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9_5KcmWVSLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "def minimum_bounding_rectangle(points):\n",
        "    \"\"\"\n",
        "    Find the smallest bounding rectangle for a set of points.\n",
        "    Returns a set of points representing the corners of the bounding box.\n",
        "\n",
        "    :param points: an nx2 matrix of coordinates\n",
        "    :rval: an nx2 matrix of coordinates\n",
        "    \"\"\"\n",
        "    from scipy.ndimage.interpolation import rotate\n",
        "    pi2 = np.pi/2.\n",
        "\n",
        "    # get the convex hull for the points\n",
        "    hull_points = points[ConvexHull(points).vertices]\n",
        "\n",
        "    # calculate edge angles\n",
        "    edges = np.zeros((len(hull_points)-1, 2))\n",
        "    edges = hull_points[1:] - hull_points[:-1]\n",
        "\n",
        "    angles = np.zeros((len(edges)))\n",
        "    angles = np.arctan2(edges[:, 1], edges[:, 0])\n",
        "\n",
        "    angles = np.abs(np.mod(angles, pi2))\n",
        "    angles = np.unique(angles)\n",
        "\n",
        "    # find rotation matrices\n",
        "    # XXX both work\n",
        "    rotations = np.vstack([\n",
        "        np.cos(angles),\n",
        "        np.cos(angles-pi2),\n",
        "        np.cos(angles+pi2),\n",
        "        np.cos(angles)]).T\n",
        "#     rotations = np.vstack([\n",
        "#         np.cos(angles),\n",
        "#         -np.sin(angles),\n",
        "#         np.sin(angles),\n",
        "#         np.cos(angles)]).T\n",
        "    rotations = rotations.reshape((-1, 2, 2))\n",
        "\n",
        "    # apply rotations to the hull\n",
        "    rot_points = np.dot(rotations, hull_points.T)\n",
        "\n",
        "    # find the bounding points\n",
        "    min_x = np.nanmin(rot_points[:, 0], axis=1)\n",
        "    max_x = np.nanmax(rot_points[:, 0], axis=1)\n",
        "    min_y = np.nanmin(rot_points[:, 1], axis=1)\n",
        "    max_y = np.nanmax(rot_points[:, 1], axis=1)\n",
        "\n",
        "    # find the box with the best area\n",
        "    areas = (max_x - min_x) * (max_y - min_y)\n",
        "    best_idx = np.argmin(areas)\n",
        "\n",
        "    # return the best box\n",
        "    x1 = max_x[best_idx]\n",
        "    x2 = min_x[best_idx]\n",
        "    y1 = max_y[best_idx]\n",
        "    y2 = min_y[best_idx]\n",
        "    r = rotations[best_idx]\n",
        "\n",
        "    rval = np.zeros((4, 2))\n",
        "    rval[0] = np.dot([x1, y2], r)\n",
        "    rval[1] = np.dot([x2, y2], r)\n",
        "    rval[2] = np.dot([x2, y1], r)\n",
        "    rval[3] = np.dot([x1, y1], r)\n",
        "\n",
        "    return rval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnT3aAiIWAh5",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whnb0MmbWDB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras import Model, Input\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, BatchNormalization, Activation, AveragePooling2D, Add, \\\n",
        "    Concatenate\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def deep_train(images, y, param=None):\n",
        "    print(param)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(images,\n",
        "                                                        y,\n",
        "                                                        test_size=0.2,\n",
        "                                                        stratify=y,\n",
        "                                                        random_state=100)\n",
        "    x_train = np.array(x_train)\n",
        "    x_test = np.array(x_test)\n",
        "\n",
        "    image_size = x_train.shape[1]\n",
        "    x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "    x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "\n",
        "    # num_filters = param[\"filter\"]\n",
        "    # num_filters2 = param[\"filter2\"]\n",
        "\n",
        "    num_filters = 2\n",
        "    num_filters2 = 18\n",
        "\n",
        "    kernel = param[\"kernel\"]\n",
        "    \n",
        "    inputs = Input(shape=(image_size, image_size, 1))\n",
        "\n",
        "    out = Conv2D(filters=num_filters,\n",
        "                kernel_size=(kernel, kernel),\n",
        "                padding=\"same\")(inputs)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=2 * num_filters,\n",
        "                kernel_size=(kernel, kernel),\n",
        "                padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=4 * num_filters,\n",
        "                kernel_size=(kernel, kernel),\n",
        "                padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = MaxPooling2D(strides=2, pool_size=2)(out)\n",
        "\n",
        "    out = Conv2D(filters=8 * num_filters,\n",
        "                kernel_size=(kernel, kernel),\n",
        "                padding=\"same\")(out)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "\n",
        "    # layer 2\n",
        "    out2 = Conv2D(filters=num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(inputs)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=2 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=4 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    out2 = MaxPooling2D(strides=2, pool_size=2)(out2)\n",
        "\n",
        "    out2 = Conv2D(filters=8 * num_filters2,\n",
        "                  kernel_size=(kernel, kernel),\n",
        "                  padding=\"same\")(out2)\n",
        "    out2 = BatchNormalization()(out2)\n",
        "    out2 = Activation('relu')(out2)\n",
        "    # final layer\n",
        "    outf = Concatenate()([out, out2])\n",
        "    out_f = AveragePooling2D(strides=2, pool_size=2)(outf)\n",
        "    out_f = Flatten()(out_f)\n",
        "    predictions = Dense(2, activation='softmax')(out_f)\n",
        "\n",
        "    # This creates a model that includes\n",
        "    # the Input layer and three Dense layers\n",
        "    model = Model(inputs=inputs, outputs=predictions)\n",
        "\n",
        "    adam = Adam(lr=param[\"learning_rate\"])\n",
        "\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=adam,\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    hist = model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=40,\n",
        "        verbose=2,\n",
        "        validation_data=(x_test, y_test),\n",
        "        batch_size=param[\"batch\"],\n",
        "        callbacks=[EarlyStopping(monitor='val_acc', mode='max', patience=10)]\n",
        "\n",
        "    )\n",
        "\n",
        "      # score = hist.history[\"accuracy\"][-1]\n",
        "      # print(\"Accuracy: \" + str(100.0 * score))\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqWkJq35Vy07",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maXgWvcPV6jj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "import numpy as np\n",
        "from hyperopt import STATUS_OK\n",
        "from hyperopt import tpe, hp, Trials, fmin\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, balanced_accuracy_score\n",
        "\n",
        "XGlobal = []\n",
        "YGlobal = []\n",
        "\n",
        "XTestGlobal = []\n",
        "YTestGlobal = []\n",
        "\n",
        "SavedParameters = []\n",
        "\n",
        "\n",
        "def hyperopt_fcn(params):\n",
        "    global SavedParameters\n",
        "    start = timeit.timeit()\n",
        "    # for p in SavedParameters:\n",
        "    #     if p[\"filter\"] == params[\"filter\"] and p[\"filter2\"] == params[\"filter2\"] and p[\"kernel\"] == params[\"kernel\"] and\\\n",
        "    #             p[\"learning_rate\"] == params[\"learning_rate\"] and p[\"momentum\"] == params[\"momentum\"]:\n",
        "    #         return {'loss': np.inf, 'status': STATUS_OK}\n",
        "    print(\"start train\")\n",
        "    model = deep_train(XGlobal, YGlobal, params)\n",
        "    print(\"start predict\")\n",
        "\n",
        "    Y_predicted = model.predict(XTestGlobal, verbose=0, use_multiprocessing=True, workers=12)\n",
        "    Y_predicted = np.argmax(Y_predicted, axis=1)\n",
        "    end = timeit.timeit()\n",
        "\n",
        "    cf = confusion_matrix(YTestGlobal, Y_predicted)\n",
        "    print(cf)\n",
        "    print(balanced_accuracy_score(YTestGlobal, Y_predicted))\n",
        "    K.clear_session()\n",
        "    # SavedParameters.append(\n",
        "    #     {\"balanced_accuracy\": balanced_accuracy_score(YTestGlobal, Y_predicted) * 100, \"TN\": cf[0][0],\n",
        "    #      \"FP\": cf[0][1], \"FN\": cf[1][0], \"TP\": cf[1][1], \"filter\": params[\"filter\"], \"filter2\": params[\"filter2\"],\n",
        "    #      \"kernel\": params[\"kernel\"], \"learning_rate\": params[\"learning_rate\"], \"momentum\": params[\"momentum\"]})\n",
        "    SavedParameters.append(\n",
        "        {\"balanced_accuracy\": balanced_accuracy_score(YTestGlobal, Y_predicted) * 100, \"TN\": cf[0][0],\n",
        "         \"FP\": cf[0][1], \"FN\": cf[1][0], \"TP\": cf[1][1], \"kernel\": params[\"kernel\"],\n",
        "         \"learning_rate\": params[\"learning_rate\"],\n",
        "         \"batch\": params[\"batch\"], \"time\": end - start\n",
        "         })\n",
        "\n",
        "    try:\n",
        "        with open(\"/content/drive/My Drive/Tesi/param.csv\", 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=SavedParameters[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(SavedParameters)\n",
        "    except IOError:\n",
        "        print(\"I/O error\")\n",
        "\n",
        "    return {'loss': -balanced_accuracy_score(YTestGlobal, Y_predicted), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "def train_norm(param, dataset, norm):\n",
        "    np.random.seed(param[\"seed\"])\n",
        "    print(\"modelling dataset\")\n",
        "    global YGlobal\n",
        "    YGlobal = to_categorical(dataset[\"Classification\"])\n",
        "    del dataset[\"Classification\"]\n",
        "    global YTestGlobal\n",
        "    YTestGlobal = to_categorical(dataset[\"Ytest\"])\n",
        "    del dataset[\"Ytest\"]\n",
        "\n",
        "    global XGlobal\n",
        "    global XTestGlobal\n",
        "\n",
        "    if not param[\"LoadFromJson\"]:\n",
        "        # norm\n",
        "        Out = {}\n",
        "        if norm:\n",
        "            print('NORM Min-Max')\n",
        "            Out[\"Max\"] = float(dataset[\"Xtrain\"].max().max())\n",
        "            Out[\"Min\"] = float(dataset[\"Xtrain\"].min().min())\n",
        "            # NORM\n",
        "            dataset[\"Xtrain\"] = (dataset[\"Xtrain\"] - Out[\"Min\"]) / (Out[\"Max\"] - Out[\"Min\"])\n",
        "            dataset[\"Xtrain\"] = dataset[\"Xtrain\"].fillna(0)\n",
        "\n",
        "        # TODO implement norm 2\n",
        "        print(\"trasposing\")\n",
        "\n",
        "        q = {\"data\": np.array(dataset[\"Xtrain\"].values).transpose(), \"method\": param[\"Metod\"],\n",
        "             \"max_px_size\": param[\"Max_P_Size\"]}\n",
        "        print(q[\"method\"])\n",
        "        print(q[\"max_px_size\"])\n",
        "\n",
        "        # generate images\n",
        "        XGlobal, image_model = Cart2Pixel(q, q[\"max_px_size\"], q[\"max_px_size\"], param[\"Dynamic_Size\"])\n",
        "        del q\n",
        "        print(\"Train Images done!\")\n",
        "        # generate testingset image\n",
        "        dataset[\"Xtest\"] = np.array(dataset[\"Xtest\"]).transpose()\n",
        "        print(\"generating Test Images\")\n",
        "        print(dataset[\"Xtest\"].shape)\n",
        "\n",
        "        XTestGlobal = [ConvPixel(dataset[\"Xtest\"][:, i], np.array(image_model[\"xp\"]), np.array(image_model[\"yp\"]),\n",
        "                                 image_model[\"A\"], image_model[\"B\"]) for i in range(0, dataset[\"Xtest\"].shape[1])]\n",
        "        print(\"Test Images done!\")\n",
        "\n",
        "        del dataset[\"Xtest\"]\n",
        "\n",
        "        # saving testingset\n",
        "        filename = \"/content/drive/My Drive/Tesi/testingsetImage.pickle\"\n",
        "        f_myfile = open(filename, 'wb')\n",
        "        pickle.dump(XTestGlobal, f_myfile)\n",
        "        f_myfile.close()\n",
        "\n",
        "    else:\n",
        "        XGlobal = dataset[\"Xtrain\"]\n",
        "        XTestGlobal = dataset[\"Xtest\"]\n",
        "    del dataset[\"Xtrain\"]\n",
        "    del dataset[\"Xtest\"]\n",
        "\n",
        "    XTestGlobal = np.array(XTestGlobal)\n",
        "    image_size = XTestGlobal.shape[1]\n",
        "    XTestGlobal = np.reshape(XTestGlobal, [-1, image_size, image_size, 1])\n",
        "    YTestGlobal = np.argmax(YTestGlobal, axis=1)\n",
        "\n",
        "    # optimizable_variable = {\"filter_size\": 3, \"kernel\": 2, \"filter_size2\": 6,\"learning_rate\":1e-5,\"momentum\":0.8}\n",
        "    optimizable_variable = {\"kernel\": hp.choice(\"kernel\", np.arange(2, 16 + 1)),\n",
        "                            \"batch\": hp.choice(\"batch\", [64, 128, 256, 512]),\n",
        "                            \"learning_rate\": hp.uniform(\"learning_rate\", 0.0001, 0.01)}\n",
        "\n",
        "    trials = Trials()\n",
        "    global SavedParameters\n",
        "    SavedParameters150 = []\n",
        "    fmin(hyperopt_fcn, optimizable_variable, trials=trials, algo=tpe.suggest, max_evals=20)\n",
        "\n",
        "    print(\"migliori parametri\")\n",
        "    SavedParameters = sorted(SavedParameters, key=lambda i: i['balanced_accuracy'], reverse=True)\n",
        "    print(SavedParameters[0])\n",
        "\n",
        "    # returning best model with hyperopt parameters\n",
        "    model = deep_train(XGlobal, YGlobal, SavedParameters[0])\n",
        "    print(\"accuracy\" + str(model))\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEpn-E17q2WE",
        "colab_type": "text"
      },
      "source": [
        "## Load data from drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RzVl55wf-be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param = {\"Max_P_Size\": 33, \"Dynamic_Size\": False, 'Metod': 'tSNE', \"ValidRatio\": 0.1, \"seed\": 180, \"Mode\": \"neural\",\n",
        "         \"LoadFromJson\": True}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHkAbMYrV0FG",
        "colab_type": "code",
        "outputId": "e1ff57b2-5180-45e2-e173-87f7a5b4ae88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not param[\"LoadFromJson\"]:\n",
        "  with open('/content/drive/My Drive/Tesi/TrainOneCls.csv', 'r') as file:\n",
        "    data = {\"Xtrain\": pd.DataFrame(list(csv.DictReader(file))).astype(float), \"class\": 2}\n",
        "    print(data[\"Xtrain\"].shape)\n",
        "    data[\"Classification\"] = data[\"Xtrain\"][\"Classification\"]\n",
        "    del data[\"Xtrain\"][\"Classification\"]\n",
        "  print(\"train loaded\")\n",
        "\n",
        "  with open('/content/drive/My Drive/Tesi/Testing.csv', 'r') as file:\n",
        "    data[\"Xtest\"]=pd.DataFrame(list(csv.DictReader(file))).astype(float)\n",
        "    data[\"Ytest\"] = data[\"Xtest\"][\"Classification\"]\n",
        "    del data[\"Xtest\"][\"Classification\"]\n",
        "    del data[\"Xtest\"]['']\n",
        "  print(\"test loaded\")\n",
        "\n",
        "  \n",
        "else:\n",
        "    images={}\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/trainingsetImage.pickle', 'rb')\n",
        "    images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/y_trainingset.pickle', 'rb')\n",
        "    images[\"Classification\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/testingsetImage.pickle', 'rb')\n",
        "    images[\"Xtest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/y_testingset.pickle', 'rb')\n",
        "    images[\"Ytest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6zf3sNZEvbg7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "236fb0c2-59ee-474e-cdc7-e37d6cacb9c9"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "if not param[\"LoadFromJson\"]:\n",
        "  with open('/content/drive/My Drive/Tesi/TrainOneCls.csv', 'r') as file:\n",
        "    data = {\"Xtrain\": pd.DataFrame(list(csv.DictReader(file))).astype(float), \"class\": 2}\n",
        "    print(data[\"Xtrain\"].shape)\n",
        "    data[\"Classification\"] = data[\"Xtrain\"][\"Classification\"]\n",
        "    del data[\"Xtrain\"][\"Classification\"]\n",
        "  print(\"train loaded\")\n",
        "\n",
        "  with open('/content/drive/My Drive/Tesi/Testing.csv', 'r') as file:\n",
        "    data[\"Xtest\"]=pd.DataFrame(list(csv.DictReader(file))).astype(float)\n",
        "    data[\"Ytest\"] = data[\"Xtest\"][\"Classification\"]\n",
        "    del data[\"Xtest\"][\"Classification\"]\n",
        "    del data[\"Xtest\"]['']\n",
        "\n",
        "  filename = \"/content/drive/My Drive/Tesi/y_testingset.pickle\"\n",
        "  f_myfile = open(filename, 'wb')\n",
        "  pickle.dump(data[\"Ytest\"], f_myfile)\n",
        "  f_myfile.close()\n",
        "\n",
        "  print(\"test loaded\")\n",
        "else:\n",
        "    images={}\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/trainingsetImage.pickle', 'rb')\n",
        "    images[\"Xtrain\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/y_trainingset.pickle', 'rb')\n",
        "    images[\"Classification\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/testingsetImage.pickle', 'rb')\n",
        "    images[\"Xtest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n",
        "    f_myfile = open('/content/drive/My Drive/Tesi/y_testingset.pickle', 'rb')\n",
        "    images[\"Ytest\"] = pickle.load(f_myfile)\n",
        "    f_myfile.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK_gYFBss4ik",
        "colab_type": "text"
      },
      "source": [
        "## Run\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idtoPR3ma_lq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = train_norm(param, images, norm=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aSDsdLyeqdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}